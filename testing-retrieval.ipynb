{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":10117530,"sourceType":"datasetVersion","datasetId":6242437},{"sourceId":10233816,"sourceType":"datasetVersion","datasetId":6327802},{"sourceId":10234185,"sourceType":"datasetVersion","datasetId":6328060},{"sourceId":10289612,"sourceType":"datasetVersion","datasetId":6368057},{"sourceId":10291358,"sourceType":"datasetVersion","datasetId":6369187},{"sourceId":10304094,"sourceType":"datasetVersion","datasetId":6378225},{"sourceId":10340315,"sourceType":"datasetVersion","datasetId":6402932},{"sourceId":10340446,"sourceType":"datasetVersion","datasetId":6403040},{"sourceId":10340612,"sourceType":"datasetVersion","datasetId":6403158},{"sourceId":10342383,"sourceType":"datasetVersion","datasetId":6404429},{"sourceId":197746,"sourceType":"modelInstanceVersion","modelInstanceId":168655,"modelId":191008},{"sourceId":210696,"sourceType":"modelInstanceVersion","modelInstanceId":179619,"modelId":201889},{"sourceId":215881,"sourceType":"modelInstanceVersion","modelInstanceId":184047,"modelId":206225},{"sourceId":216111,"sourceType":"modelInstanceVersion","modelInstanceId":184244,"modelId":206414}],"dockerImageVersionId":30823,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install sentence_transformers","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-03T05:43:01.182754Z","iopub.execute_input":"2025-01-03T05:43:01.183027Z","iopub.status.idle":"2025-01-03T05:43:06.096660Z","shell.execute_reply.started":"2025-01-03T05:43:01.182997Z","shell.execute_reply":"2025-01-03T05:43:06.095854Z"}},"outputs":[{"name":"stdout","text":"Collecting sentence_transformers\n  Downloading sentence_transformers-3.3.1-py3-none-any.whl.metadata (10 kB)\nRequirement already satisfied: transformers<5.0.0,>=4.41.0 in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (4.44.2)\nRequirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (4.66.5)\nRequirement already satisfied: torch>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (2.4.1+cu121)\nRequirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (1.2.2)\nRequirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (1.13.1)\nRequirement already satisfied: huggingface-hub>=0.20.0 in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (0.24.7)\nRequirement already satisfied: Pillow in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (10.4.0)\nRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.20.0->sentence_transformers) (3.16.1)\nRequirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.20.0->sentence_transformers) (2024.6.1)\nRequirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.20.0->sentence_transformers) (24.1)\nRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.20.0->sentence_transformers) (6.0.2)\nRequirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.20.0->sentence_transformers) (2.32.3)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.20.0->sentence_transformers) (4.12.2)\nRequirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence_transformers) (1.13.3)\nRequirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence_transformers) (3.3)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence_transformers) (3.1.4)\nRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.41.0->sentence_transformers) (1.26.4)\nRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.41.0->sentence_transformers) (2024.9.11)\nRequirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.41.0->sentence_transformers) (0.4.5)\nRequirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.41.0->sentence_transformers) (0.19.1)\nRequirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->sentence_transformers) (1.4.2)\nRequirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->sentence_transformers) (3.5.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.11.0->sentence_transformers) (2.1.5)\nRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.20.0->sentence_transformers) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.20.0->sentence_transformers) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.20.0->sentence_transformers) (2.2.3)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.20.0->sentence_transformers) (2024.8.30)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.11.0->sentence_transformers) (1.3.0)\nDownloading sentence_transformers-3.3.1-py3-none-any.whl (268 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m268.8/268.8 kB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n\u001b[?25hInstalling collected packages: sentence_transformers\nSuccessfully installed sentence_transformers-3.3.1\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"# !pip install py_vncorenlp","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-02T16:09:08.967949Z","iopub.execute_input":"2025-01-02T16:09:08.968260Z","iopub.status.idle":"2025-01-02T16:09:08.972281Z","shell.execute_reply.started":"2025-01-02T16:09:08.968227Z","shell.execute_reply":"2025-01-02T16:09:08.971304Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"from sentence_transformers import SentenceTransformer, CrossEncoder\nimport pandas as pd\nfrom sentence_transformers import util","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-03T05:43:10.794732Z","iopub.execute_input":"2025-01-03T05:43:10.795054Z","iopub.status.idle":"2025-01-03T05:43:25.447667Z","shell.execute_reply.started":"2025-01-03T05:43:10.795027Z","shell.execute_reply":"2025-01-03T05:43:25.446957Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"# import py_vncorenlp\n# rdrsegmenter = py_vncorenlp.VnCoreNLP(annotators=[\"wseg\"], save_dir='/kaggle/input/vncorenlp/VnCoreNLP-master')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-02T16:09:22.038173Z","iopub.execute_input":"2025-01-02T16:09:22.038689Z","iopub.status.idle":"2025-01-02T16:09:22.041810Z","shell.execute_reply.started":"2025-01-02T16:09:22.038665Z","shell.execute_reply":"2025-01-02T16:09:22.041102Z"}},"outputs":[],"execution_count":4},{"cell_type":"markdown","source":"# Models","metadata":{}},{"cell_type":"code","source":"from sentence_transformers import SentenceTransformer\nimport torch\n\n# Kiểm tra xem GPU có sẵn không\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\nprint(f\"Using device: {device}\")\n\n# Tải mô hình và chỉ định thiết bị\nmodel_path = \"/kaggle/input/m/ictunivers/halong_embedding/transformers/default/1/checkpoint-2500\"\nmodel = SentenceTransformer(model_path, device=device)\n\n# Kiểm tra xem model đã được tải trên GPU hay không\nprint(f\"Model device: {model.device}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-03T05:43:31.474770Z","iopub.execute_input":"2025-01-03T05:43:31.475352Z","iopub.status.idle":"2025-01-03T05:43:46.546820Z","shell.execute_reply.started":"2025-01-03T05:43:31.475321Z","shell.execute_reply":"2025-01-03T05:43:46.545870Z"}},"outputs":[{"name":"stdout","text":"Using device: cuda\nModel device: cuda:0\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"print(model)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-03T05:43:49.697920Z","iopub.execute_input":"2025-01-03T05:43:49.698211Z","iopub.status.idle":"2025-01-03T05:43:49.702801Z","shell.execute_reply.started":"2025-01-03T05:43:49.698190Z","shell.execute_reply":"2025-01-03T05:43:49.701892Z"}},"outputs":[{"name":"stdout","text":"SentenceTransformer(\n  (0): Transformer({'max_seq_length': 512, 'do_lower_case': False}) with Transformer model: XLMRobertaModel \n  (1): Pooling({'word_embedding_dimension': 768, 'pooling_mode_cls_token': False, 'pooling_mode_mean_tokens': True, 'pooling_mode_max_tokens': False, 'pooling_mode_mean_sqrt_len_tokens': False, 'pooling_mode_weightedmean_tokens': False, 'pooling_mode_lasttoken': False, 'include_prompt': True})\n  (2): Normalize()\n)\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"from sentence_transformers import CrossEncoder\nimport torch\n\n# Kiểm tra xem GPU có sẵn không\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\nprint(f\"Using device: {device}\")\n\n# Tải mô hình và chỉ định thiết bị\nmodel_name = \"BAAI/bge-reranker-v2-m3\"\ncross_encoder = CrossEncoder(model_name, num_labels=1, max_length=1024, device=device)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-03T05:43:53.508109Z","iopub.execute_input":"2025-01-03T05:43:53.508383Z","iopub.status.idle":"2025-01-03T05:44:49.248776Z","shell.execute_reply.started":"2025-01-03T05:43:53.508361Z","shell.execute_reply":"2025-01-03T05:44:49.247923Z"}},"outputs":[{"name":"stdout","text":"Using device: cuda\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/795 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"bcbd2b5f9af34bd7bf00d17d50b0c891"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/2.27G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6a509bde58114e158e5626b2630bae03"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/1.17k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"55bafe7b284f48a98fc9b009d7bbe5f9"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"sentencepiece.bpe.model:   0%|          | 0.00/5.07M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b83590b617a8447ea7da945803843c95"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/17.1M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1d7321016b2a4587876d4c699b0eb5c2"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/964 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8f745edb946d4109939628fe0296ed5c"}},"metadata":{}}],"execution_count":5},{"cell_type":"code","source":"# model_name = \"/kaggle/input/phoranker_finetune_jan/transformers/default/1/phoranker_finetune_Dec\"\n# cross_encoder = CrossEncoder(model_name, num_labels=1, max_length=256)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-02T16:10:35.399818Z","iopub.execute_input":"2025-01-02T16:10:35.400016Z","iopub.status.idle":"2025-01-02T16:10:35.403034Z","shell.execute_reply.started":"2025-01-02T16:10:35.399997Z","shell.execute_reply":"2025-01-02T16:10:35.402377Z"}},"outputs":[],"execution_count":8},{"cell_type":"code","source":"import logging\nlogging.disable(logging.WARNING)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-03T05:47:48.848844Z","iopub.execute_input":"2025-01-03T05:47:48.849169Z","iopub.status.idle":"2025-01-03T05:47:48.853578Z","shell.execute_reply.started":"2025-01-03T05:47:48.849149Z","shell.execute_reply":"2025-01-03T05:47:48.852689Z"}},"outputs":[],"execution_count":6},{"cell_type":"code","source":"model_cross = cross_encoder.model\n\n# Print the model architecture\nprint(model_cross)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-03T05:47:53.035742Z","iopub.execute_input":"2025-01-03T05:47:53.035956Z","iopub.status.idle":"2025-01-03T05:47:53.041918Z","shell.execute_reply.started":"2025-01-03T05:47:53.035933Z","shell.execute_reply":"2025-01-03T05:47:53.041115Z"}},"outputs":[{"name":"stdout","text":"XLMRobertaForSequenceClassification(\n  (roberta): XLMRobertaModel(\n    (embeddings): XLMRobertaEmbeddings(\n      (word_embeddings): Embedding(250002, 1024, padding_idx=1)\n      (position_embeddings): Embedding(8194, 1024, padding_idx=1)\n      (token_type_embeddings): Embedding(1, 1024)\n      (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n      (dropout): Dropout(p=0.1, inplace=False)\n    )\n    (encoder): XLMRobertaEncoder(\n      (layer): ModuleList(\n        (0-23): 24 x XLMRobertaLayer(\n          (attention): XLMRobertaAttention(\n            (self): XLMRobertaSelfAttention(\n              (query): Linear(in_features=1024, out_features=1024, bias=True)\n              (key): Linear(in_features=1024, out_features=1024, bias=True)\n              (value): Linear(in_features=1024, out_features=1024, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): XLMRobertaSelfOutput(\n              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): XLMRobertaIntermediate(\n            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n            (intermediate_act_fn): GELUActivation()\n          )\n          (output): XLMRobertaOutput(\n            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n      )\n    )\n  )\n  (classifier): XLMRobertaClassificationHead(\n    (dense): Linear(in_features=1024, out_features=1024, bias=True)\n    (dropout): Dropout(p=0.1, inplace=False)\n    (out_proj): Linear(in_features=1024, out_features=1, bias=True)\n  )\n)\n","output_type":"stream"}],"execution_count":7},{"cell_type":"markdown","source":"# Corpus","metadata":{}},{"cell_type":"code","source":"corpus = pd.read_csv(\"/kaggle/input/healthcare/Corpus (1).csv\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-03T05:48:03.037846Z","iopub.execute_input":"2025-01-03T05:48:03.038067Z","iopub.status.idle":"2025-01-03T05:48:03.441707Z","shell.execute_reply.started":"2025-01-03T05:48:03.038046Z","shell.execute_reply":"2025-01-03T05:48:03.440861Z"}},"outputs":[],"execution_count":9},{"cell_type":"code","source":"print(corpus.info())","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-03T05:48:07.335902Z","iopub.execute_input":"2025-01-03T05:48:07.336121Z","iopub.status.idle":"2025-01-03T05:48:07.363118Z","shell.execute_reply.started":"2025-01-03T05:48:07.336097Z","shell.execute_reply":"2025-01-03T05:48:07.362380Z"}},"outputs":[{"name":"stdout","text":"<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 25668 entries, 0 to 25667\nData columns (total 1 columns):\n #   Column  Non-Null Count  Dtype \n---  ------  --------------  ----- \n 0   corpus  25668 non-null  object\ndtypes: object(1)\nmemory usage: 200.7+ KB\nNone\n","output_type":"stream"}],"execution_count":10},{"cell_type":"code","source":"# import pandas as pd\n# import h5py\n# from tqdm import tqdm\n# import numpy as np\n\n\n# # Extract texts \n# texts = corpus[\"context\"].tolist() \n# cids = corpus['cid'].tolist()\n\n\n# batch_size = 128  # Adjust this based on your system's memory\n# embeddings = []\n# model = model.to('cuda')\n# print(model.device)\n# print(\"Encoding text...\")\n# for start_idx in tqdm(range(0, len(texts), batch_size)):\n#     batch_texts = texts[start_idx:start_idx + batch_size]\n#     batch_embeddings = model.encode(batch_texts, convert_to_numpy=True, show_progress_bar=False)\n#     embeddings.append(batch_embeddings)\n\n# embeddings = np.vstack(embeddings)\n# print(\"start embeddings\")\n# output_file = \"/kaggle/working/corpus_embeddings_halong_03.h5\"  \n# print(f\"Saving embeddings to {output_file}...\")\n# with h5py.File(output_file, \"w\") as f:\n#     f.create_dataset(\"embeddings\", data=embeddings)\n#     f.create_dataset(\"cids\", data=np.array(cids, dtype=\"S\")) \n\n# print(\"Embeddings saved successfully.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-02T16:10:47.701727Z","iopub.execute_input":"2025-01-02T16:10:47.701922Z","iopub.status.idle":"2025-01-02T16:10:47.705294Z","shell.execute_reply.started":"2025-01-02T16:10:47.701893Z","shell.execute_reply":"2025-01-02T16:10:47.704607Z"}},"outputs":[],"execution_count":13},{"cell_type":"markdown","source":"# Vector embeddings call ","metadata":{}},{"cell_type":"code","source":"import h5py\n\n# Đường dẫn đến tệp HDF5\nfile_path = \"/kaggle/input/healthcare/doc_embeddings.pt\"\n\n# Mở tệp HDF5\nwith h5py.File(file_path, \"r\") as f:\n    # Hiển thị các nhóm trong tệp\n    print(\"Các nhóm trong tệp HDF5:\", list(f.keys()))\n\n    # Lấy nhóm cids và embeddings\n    cids = f['cids'][:]  # Lấy toàn bộ dữ liệu trong nhóm cids\n    embeddings = f['embeddings'][:]  # Lấy toàn bộ dữ liệu trong nhóm embeddings\n\n    # Lấy một mẫu từ nhóm cids và nhóm embeddings\n    cid_sample = cids[0]  # Ví dụ: lấy mẫu đầu tiên từ cids\n    embedding_sample = embeddings[0]  # Lấy mẫu đầu tiên từ embeddings\n\n    # Hiển thị kết quả\n    print(\"Sample từ nhóm cids:\", cid_sample)  # Giải mã chuỗi byte\n    print(\"Sample từ nhóm embeddings:\", embedding_sample)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-03T05:48:40.567516Z","iopub.execute_input":"2025-01-03T05:48:40.567747Z","iopub.status.idle":"2025-01-03T05:48:41.038617Z","shell.execute_reply.started":"2025-01-03T05:48:40.567727Z","shell.execute_reply":"2025-01-03T05:48:41.037390Z"}},"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)","\u001b[0;32m<ipython-input-11-2ca2c1a6dda9>\u001b[0m in \u001b[0;36m<cell line: 7>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;31m# Mở tệp HDF5\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0;32mwith\u001b[0m \u001b[0mh5py\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"r\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m     \u001b[0;31m# Hiển thị các nhóm trong tệp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Các nhóm trong tệp HDF5:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/h5py/_hl/files.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, name, mode, driver, libver, userblock_size, swmr, rdcc_nslots, rdcc_nbytes, rdcc_w0, track_order, fs_strategy, fs_persist, fs_threshold, fs_page_size, page_buf_size, min_meta_keep, min_raw_keep, locking, alignment_threshold, alignment_interval, meta_block_size, **kwds)\u001b[0m\n\u001b[1;32m    560\u001b[0m                                  \u001b[0mfs_persist\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfs_persist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfs_threshold\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfs_threshold\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    561\u001b[0m                                  fs_page_size=fs_page_size)\n\u001b[0;32m--> 562\u001b[0;31m                 \u001b[0mfid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmake_fid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muserblock_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfapl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfcpl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mswmr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mswmr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    563\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    564\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlibver\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/h5py/_hl/files.py\u001b[0m in \u001b[0;36mmake_fid\u001b[0;34m(name, mode, userblock_size, fapl, fcpl, swmr)\u001b[0m\n\u001b[1;32m    233\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mswmr\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mswmr_support\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    234\u001b[0m             \u001b[0mflags\u001b[0m \u001b[0;34m|=\u001b[0m \u001b[0mh5f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mACC_SWMR_READ\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 235\u001b[0;31m         \u001b[0mfid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mh5f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflags\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfapl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfapl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    236\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mmode\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'r+'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    237\u001b[0m         \u001b[0mfid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mh5f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh5f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mACC_RDWR\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfapl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfapl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32mh5py/_objects.pyx\u001b[0m in \u001b[0;36mh5py._objects.with_phil.wrapper\u001b[0;34m()\u001b[0m\n","\u001b[0;32mh5py/_objects.pyx\u001b[0m in \u001b[0;36mh5py._objects.with_phil.wrapper\u001b[0;34m()\u001b[0m\n","\u001b[0;32mh5py/h5f.pyx\u001b[0m in \u001b[0;36mh5py.h5f.open\u001b[0;34m()\u001b[0m\n","\u001b[0;31mOSError\u001b[0m: Unable to synchronously open file (file signature not found)"],"ename":"OSError","evalue":"Unable to synchronously open file (file signature not found)","output_type":"error"}],"execution_count":11},{"cell_type":"code","source":"import torch\n\n# Đường dẫn đến tệp .pt\nfile_path = \"/kaggle/input/healthcare/doc_embeddings.pt\"\n\n# Tải dữ liệu\ndata = torch.load(file_path)\n\n# Kiểm tra và xử lý dữ liệu\nif isinstance(data, torch.Tensor):\n    # Nếu toàn bộ file là một tensor\n    embeddings = data\nelif isinstance(data, dict) and 'embeddings' in data:\n    # Nếu file là dictionary và chứa key 'embeddings'\n    embeddings = data['embeddings']\nelse:\n    raise ValueError(\"File không chứa embeddings hoặc không đúng định dạng!\")\n\n# Hiển thị thông tin embeddings\nprint(f\"Tổng số embeddings: {len(embeddings)}\")\nprint(\"Embedding mẫu:\", embeddings[0])\n\n# Đảm bảo kiểu dữ liệu là tensor\nif isinstance(embeddings, torch.Tensor):\n    print(f\"Kích thước của embedding: {embeddings.shape}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-03T06:01:27.548362Z","iopub.execute_input":"2025-01-03T06:01:27.548601Z","iopub.status.idle":"2025-01-03T06:01:27.659135Z","shell.execute_reply.started":"2025-01-03T06:01:27.548574Z","shell.execute_reply":"2025-01-03T06:01:27.658142Z"}},"outputs":[{"name":"stdout","text":"Tổng số embeddings: 25668\nEmbedding mẫu: tensor([-2.5561e-01, -2.2460e-01,  2.7973e-01,  2.9836e-01,  5.6402e-03,\n         3.0811e-01, -9.0863e-03,  2.9652e-01, -5.1866e-02,  3.6566e-01,\n        -1.5304e-01,  7.3074e-02, -2.1834e-02,  4.5647e-04,  1.7569e-01,\n        -1.6430e-01, -4.3231e-02, -2.1705e-01, -1.1724e-01, -3.0267e-01,\n         2.7409e-01, -8.0533e-02, -1.3059e-01, -2.8147e-01,  6.0854e-01,\n        -7.5169e-02,  2.4090e-01, -1.1425e-01,  6.9873e-02, -9.1570e-03,\n        -4.5452e-02,  1.6003e-01,  3.3195e-02, -1.0014e-01, -5.3177e-01,\n         8.1638e-02,  2.9084e-02, -9.9377e-02,  1.2253e-01, -7.3335e-01,\n         7.0233e-02, -1.0116e-01,  1.6906e-01,  1.0135e-01, -1.1252e-01,\n         1.0180e-01, -1.0976e-01, -5.1516e-01, -3.3952e-01,  1.5647e-01,\n         7.7569e-02,  8.1755e-02,  1.4685e-01,  4.1210e-01,  3.0738e-01,\n        -2.3537e-01,  8.5673e-02,  6.0791e-02, -2.7032e-01,  3.8287e-02,\n         3.9097e-01,  1.3501e-01,  3.9549e-02, -5.2846e-02, -4.2594e-01,\n         3.3680e-02, -2.5877e-01, -1.0082e-02,  2.0763e-01,  3.2714e-01,\n         2.7228e-01,  2.1963e-01, -1.0882e-01, -3.5854e-01, -2.8517e-01,\n        -1.8231e-01,  1.0750e-01,  2.7685e-01, -1.2401e-01, -3.3315e-01,\n        -8.5725e-02,  3.9394e-01,  3.6094e-01,  4.7673e-02, -1.4411e-02,\n         1.9982e-01,  5.6501e-03,  1.1754e-01, -2.0696e-02,  1.2375e-01,\n        -1.6541e-01,  1.2517e-01, -1.0326e-01,  1.0857e-01, -9.6221e-02,\n         3.4988e-02,  1.2525e-02,  3.5194e-01, -2.3009e-01,  4.6834e-01,\n        -1.7682e-01,  2.3560e-01,  1.0748e-01, -1.3953e-01,  2.5598e-01,\n        -1.3753e-01,  2.7505e-01, -2.4671e-02,  1.3948e-01,  3.6649e-02,\n        -5.2813e-02, -2.2207e-02,  1.9755e-01, -5.0901e-02, -4.8073e-01,\n        -2.7571e-01, -1.0787e-01,  1.2694e-01,  2.0076e-02,  3.2307e-01,\n        -8.3162e-03,  2.2644e-01,  1.6197e-01, -9.2658e-02, -4.4644e-01,\n        -1.2793e-02,  7.3254e-02, -2.7310e-02, -6.4419e-02,  2.2995e-01,\n        -8.8139e-03, -2.5574e-01,  8.0459e-02,  6.8151e-02,  1.4919e-02,\n        -2.1589e-01, -2.8132e-01, -6.8526e-02,  2.3374e-01,  2.6949e-01,\n        -3.0498e-02,  2.1990e-01, -2.0400e-01, -6.2088e-01, -1.2446e-01,\n         1.1395e-01, -6.0155e-01,  3.8845e-02, -3.0028e-01, -3.5669e-01,\n         1.2492e-01, -4.6734e-01, -1.1262e-02,  5.0603e-01, -4.8751e-01,\n        -8.6902e-02, -2.7385e-01, -2.4943e-01, -1.8879e-01,  4.7927e-02,\n         4.5923e-01, -7.1522e-02, -1.5344e-01,  2.4938e-01,  9.4033e-02,\n         2.4660e-01, -4.1004e-01,  1.4489e-01, -8.2145e-02,  2.8792e-02,\n         5.7016e-02,  2.6895e-01, -4.8306e-01, -2.3330e-01, -5.3716e-01,\n         1.8890e-02,  3.7155e-01, -3.0120e-02, -2.5431e-01,  3.2260e-01,\n         2.2913e-01,  3.1386e-01, -1.0576e-01,  4.3387e-02, -1.1175e-01,\n         1.1675e-01, -1.7672e-01,  1.8197e-01,  1.8412e-01,  1.6979e-01,\n         2.7286e-02,  2.0644e-02,  2.0190e-01,  2.4561e-01,  8.2100e-03,\n         6.4799e-02,  2.2287e-01,  2.1750e-01,  1.2712e-02, -1.4414e-01,\n        -2.4244e-01,  2.9745e-01,  2.1285e-01,  3.2450e-01, -1.1673e-01,\n         2.5527e-01, -1.4805e-01, -3.7425e-02, -1.1241e-01, -2.5447e-01,\n        -2.5591e-01, -1.9964e-01, -1.1743e-01, -1.4837e-02,  2.7597e-01,\n        -1.6807e-01, -2.0632e-02,  1.8854e-01,  1.1474e-01,  2.9048e-01,\n         5.8883e-03,  2.9542e-02,  1.8122e-01,  3.9336e-01,  3.9327e-01,\n         7.6540e-03, -1.0117e-02,  2.0412e-02,  2.0359e-01,  6.4330e-02,\n        -1.3966e-01, -1.1540e-01,  1.0863e-01, -2.2570e-01, -1.5170e-01,\n         2.2514e-01,  2.3464e-01,  3.8443e-01,  2.1875e-03,  2.0242e-01,\n        -5.0485e-02, -2.7686e-01, -2.2365e-01, -9.4430e-02, -3.2838e-03,\n        -1.2693e-01, -8.5568e-02, -1.8718e-01, -3.8417e-01,  9.8233e-02,\n         7.9339e-02,  3.1852e-01,  3.9643e-01,  5.2728e-02, -5.5394e-02,\n         1.2038e-01,  1.0367e-01,  2.0369e-01, -2.8958e-02,  7.3308e-02,\n         6.7939e-02,  1.9884e-01, -5.0574e-02,  6.3498e-02, -1.8373e-01,\n        -1.9086e-03, -3.0505e-01,  1.3265e-01, -2.0229e-01,  1.0686e-01,\n         9.3704e-02,  2.1318e-01,  2.1301e-01, -2.2921e-02,  7.7331e-02,\n         1.4560e-02, -6.8749e-02,  2.5483e-01,  1.2372e-01,  1.8602e-02,\n        -3.0052e-01, -8.8906e-02, -2.4150e-02,  4.1372e-02, -3.8062e-01,\n         2.9738e-01,  4.4084e-02, -7.7179e-03,  9.2169e-02,  2.7732e-01,\n         4.9854e-01,  8.3106e-02,  2.8963e-02, -7.6296e-01,  1.9777e-01,\n        -5.0475e-01,  2.3351e-01, -1.9561e-01, -1.5810e-02,  5.6112e-02,\n        -1.0156e-01, -3.0639e-01,  8.9133e-03, -4.1215e-02, -2.5514e-01,\n         8.4564e-02,  8.3584e-02, -2.4122e-01, -2.1262e-01,  3.6313e-02,\n        -6.3882e-02,  7.8246e-02,  3.1138e-01, -3.5283e-02,  4.3798e-02,\n         2.4240e-02,  1.6175e-01,  9.0123e-02,  1.1601e-01,  1.6299e-01,\n        -1.6350e-01, -6.2974e-02, -8.1529e-02,  1.4457e-01, -1.1393e-01,\n        -1.7260e-01, -3.1079e-01, -3.9482e-02,  4.6245e-02, -5.5121e-02,\n         1.4671e-01, -1.4741e-01, -7.5446e-02, -3.6036e-02,  1.6458e-01,\n         1.8664e-01, -1.3683e-01,  1.5781e-01, -2.0675e-02, -5.7069e-02,\n         1.2740e-01,  4.7804e-02, -3.7783e-01,  7.9885e-02, -6.0399e-01,\n        -5.4000e-04,  1.0740e-01, -2.6322e-02,  1.8871e-01,  1.0357e-01,\n         1.6345e-01,  4.4461e-01,  1.4472e-02, -9.8121e-02,  2.2286e-01,\n         2.8378e-01,  2.1091e-01, -4.7208e-01, -1.3639e-01, -2.0124e-01,\n        -3.2254e-01, -4.1010e-02, -5.4399e-02, -3.3647e-01, -2.4878e-01,\n        -1.0162e-01, -1.3171e-01,  5.0338e-02,  1.9261e-01, -2.4364e-02,\n        -8.4158e-02,  2.1601e-01, -3.3570e-01, -1.6106e-01,  4.6431e-01,\n         2.9160e-01,  1.1552e-01,  2.6695e-01,  1.4403e-01, -5.3340e-02,\n        -3.8852e-01, -8.1510e-02,  1.8009e-01,  7.9269e-02, -3.8389e-01,\n         2.4953e-01, -3.8861e-01,  1.3398e-01,  4.4175e-01,  2.4807e-01,\n         2.7733e-01,  9.9131e-02,  7.0860e-01,  8.7449e-02,  1.5406e-01,\n        -3.6698e-01, -5.2691e-02,  1.5500e-01, -8.8411e-02,  5.6254e-01,\n         3.5634e-01, -1.2818e-01,  1.9544e-01, -9.2811e-04, -2.5953e-01,\n        -2.0846e-01,  1.9507e-01, -4.0895e-02,  2.0419e-01, -1.3353e-01,\n        -5.0213e-02, -1.9613e-01, -1.4630e-01, -4.6255e-01,  1.5234e-01,\n        -1.3862e-01,  2.5818e-01, -3.1668e-01, -5.4313e-02,  3.6845e-01,\n        -2.0315e-02, -6.4220e-02, -1.2393e-01,  1.8645e-01, -3.7442e-01,\n        -2.6376e-01, -2.4135e-02, -6.4514e-02, -3.0536e-03,  1.9843e-01,\n         2.0758e-01, -3.0687e-01,  8.7370e-02, -1.8255e-01, -4.7143e-02,\n        -9.7491e-02,  1.6482e-02,  2.8789e-01, -8.5849e-02,  2.0229e-01,\n        -6.0361e-02, -2.7388e-02, -2.5193e-01,  4.4172e-02,  9.4981e-02,\n         1.4078e-01, -1.8974e-01,  2.7703e-01, -2.0707e-02,  1.3524e-01,\n        -3.6666e-01,  1.2219e-01, -1.0056e-01,  1.5587e-01, -3.7367e-02,\n         2.4615e-01,  1.6703e-01, -1.0756e-01,  1.7617e-02, -5.3185e-04,\n        -1.4661e-01,  2.3926e-01, -3.7845e-01, -5.0849e-02, -3.8667e-01,\n         8.9029e-02, -2.3041e-01,  4.5091e-02, -2.6148e-02, -2.3368e-01,\n        -4.4216e-02, -8.7159e-02,  1.8322e-01, -1.8718e-01,  9.7739e-02,\n         5.9976e-02,  5.3838e-02, -9.7045e-02, -3.2859e-02, -2.7741e-01,\n         1.4665e-01,  1.7144e-02,  2.9873e-01, -5.4737e-02,  3.2450e-01,\n        -3.0806e-01,  2.9164e-01, -3.6438e-02, -3.3484e-01, -2.8860e-01,\n         6.8725e-02, -1.8258e-01, -3.5515e-01,  3.5422e-01,  1.1723e-01,\n         1.9074e-01,  1.6226e-01,  1.7957e-01, -3.5912e-01, -1.1392e-01,\n         2.9645e-01, -5.5895e-02, -4.5639e-02, -3.5163e-01, -1.3973e-01,\n        -3.7739e-02, -1.7578e-03,  1.1014e-01, -2.3592e-01,  9.2332e-02,\n         1.9103e-01,  2.8455e-01,  1.4505e-01,  4.8899e-02, -2.0368e-01,\n         1.7513e-01,  1.4353e-01,  9.7566e-02,  1.3608e-01,  2.6901e-01,\n         3.1071e-01,  3.8171e-01,  9.4734e-02,  1.9941e-01, -3.5039e-01,\n         1.3971e-01, -6.5939e-02,  4.1249e-01,  7.5249e-02, -1.0671e-02,\n         2.6682e-01, -1.4779e-01,  6.7486e-02, -8.6388e-02, -1.6552e-01,\n        -4.3024e-02,  1.9208e-01, -2.1318e-01, -1.5121e-01,  1.0694e-01,\n        -3.0968e-01, -2.3295e-01,  5.6242e-02, -8.8682e-02, -9.6314e-02,\n         1.3572e-01, -2.8585e-01,  4.1703e-01,  6.0631e-02, -5.8000e-02,\n        -1.3532e-01,  2.2004e-01, -1.5425e-01, -1.4744e-01, -4.7657e-02,\n         1.0025e-01, -1.6096e-01,  2.9520e-01, -4.2436e-02, -9.0630e-02,\n         1.1064e-01, -1.1184e-02, -5.8716e-02, -1.6036e-01,  2.0056e-01,\n        -7.3291e-02, -1.7825e-03, -6.5766e-02, -2.2714e-01,  7.4130e-02,\n         2.5320e-01,  2.8259e-01,  1.9158e-02,  1.6702e-01, -2.8039e-03,\n        -2.9964e-01, -1.9594e-01,  4.6633e-01, -1.9837e-01, -9.3960e-02,\n        -1.3197e-01, -1.0412e-01, -2.1236e-01, -1.4546e-01,  1.5380e-01,\n        -1.9054e-01, -2.3005e-02, -5.4817e-02,  1.1116e-01,  1.3610e-01,\n         6.3041e-02,  1.2478e-01,  3.5139e-02,  2.3310e-02, -1.0621e-01,\n         3.6453e-01, -2.4266e-01,  2.8407e-01, -1.9815e-01,  3.9854e-01,\n        -5.0127e-02, -1.6322e-01,  3.6338e-01, -6.4706e-03,  2.8418e-01,\n        -1.7811e-01,  1.0857e-01, -3.9993e-01,  1.8580e-01, -1.2343e-01,\n         1.6576e-01,  1.7985e-01,  7.6901e-02, -4.4925e-01, -1.8103e-01,\n         7.1787e-03,  1.3465e-01,  1.0604e-01, -8.7943e-02,  2.7439e-01,\n        -5.6999e-02,  7.9203e-02,  6.0416e-02, -2.0213e-02, -1.0563e-01,\n         1.3940e-02, -4.7470e-02,  2.5872e-01, -1.0331e-01,  3.1774e-02,\n        -7.1675e-02, -8.1291e-02, -1.0700e-01, -1.2543e-01, -1.7821e-01,\n        -1.7918e-01, -3.5115e-01, -8.7213e-02, -3.9632e-01, -2.7744e-01,\n        -4.7612e-01, -5.8021e-02,  4.0351e-02,  3.8289e-02, -2.6631e-01,\n         2.4646e-01,  1.1200e-01,  2.9593e-02,  1.3302e-01, -2.9758e-01,\n         2.2390e-01, -2.2083e-01,  1.0407e-02, -1.8928e-01,  2.2279e-02,\n         2.4707e-02,  5.8791e-01, -8.4692e-02,  1.9079e-01, -7.5747e-02,\n         1.0116e-01,  1.7350e-01, -2.1577e-01, -8.7837e-03,  1.9919e-02,\n        -2.8396e-01,  3.6787e-01,  2.6494e-01, -2.1033e-01, -1.9521e-01,\n         1.7615e-01,  5.6845e-03, -5.5101e-02, -2.5842e-01, -1.1775e-01,\n        -1.9411e-01, -2.6830e-02, -2.2539e-01, -1.7789e-01, -2.3627e-01,\n         2.7013e-01,  4.5518e-02,  9.8409e-02, -1.4563e-01,  2.2376e-01,\n         2.1113e-01,  2.8638e-02, -1.8991e-01,  3.2531e-01, -1.4669e-01,\n         1.2826e-01,  1.0242e-01,  2.9585e-01,  3.2270e-01,  4.9249e-02,\n        -2.2680e-01,  9.2888e-02,  3.8466e-03,  3.4797e-02,  4.0151e-01,\n         5.4439e-02,  1.3753e-01, -4.4270e-01, -1.7297e-01, -1.8881e-01,\n        -1.5054e-01, -1.3775e-01, -2.1500e-01,  3.6336e-04,  2.4470e-02,\n        -1.0848e-01, -4.1334e-02,  6.5737e-02, -8.9148e-02, -1.2594e-01,\n        -1.5473e-01, -1.7432e-01,  3.5937e-02,  1.7867e-01, -2.3675e-01,\n         9.6976e-02, -2.0728e-02, -1.1600e-01, -2.5481e-01, -1.6934e-01,\n         3.7150e-01, -6.2957e-02,  1.5108e-01,  1.1378e-01, -1.4535e-01,\n        -3.0113e-01, -3.7592e-01, -2.1108e-01,  1.4377e-01,  1.3682e-02,\n         8.9126e-02,  1.5540e-01, -1.0122e-01, -2.7889e-01,  1.6323e-01,\n        -1.0339e-01,  1.6460e-01, -5.9304e-01,  2.4600e-01, -9.2561e-02,\n         1.4262e-02, -7.6201e-02,  1.1017e-01, -7.6310e-03, -8.0805e-02,\n         6.5840e-02,  1.4663e-01,  8.9443e-02,  1.0563e-01, -1.4372e-01,\n        -1.3647e-01, -5.8007e-02,  7.7419e-02, -3.1272e-01, -2.4051e-01,\n        -1.6287e-01, -2.6660e-01,  8.5695e-02,  5.3194e-02,  2.4459e-01,\n        -4.3766e-02, -7.9769e-02, -1.4848e-01], device='cuda:0')\nKích thước của embedding: torch.Size([25668, 768])\n","output_type":"stream"},{"name":"stderr","text":"<ipython-input-13-5240e0dc9989>:7: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n  data = torch.load(file_path)\n","output_type":"stream"}],"execution_count":13},{"cell_type":"code","source":"import re\n\ndef preprocess_text(text):\n    # Replace newline characters with spaces\n    text = ' '.join(text.splitlines())\n    \n    # Replace escaped newline characters with periods\n    text = text.replace('\\\\n', '.')\n    \n    # Remove specific unwanted characters and patterns\n    text = text.replace('/', '')\n    text = text.replace(\"''\", '')\n    text = text.strip(\"[]\")\n    text = text.strip(\"'\")\n    text = text.strip('\"')\n    \n    # Remove numbered list indicators and lettered list indicators\n    text = re.sub(r'\\b\\d\\.\\s*', '', text)  # Removes patterns like '1. ', '2. ', etc.\n    text = re.sub(r'\\b[a-e]\\)\\s*', '', text)  # Removes patterns like 'a) ', 'b) ', etc.\n    \n    # Replace certain punctuation with periods\n    text = text.replace(';', '.')\n    text = text.replace(':', '.')\n    \n    # Remove hyphens\n    text = text.replace('-', '')\n    \n    # Strip leading and trailing whitespace\n    text = text.strip()\n\n    text = re.sub(r'\\s{2,}', ' ', text)\n\n    \n    return text\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-03T06:01:35.367567Z","iopub.execute_input":"2025-01-03T06:01:35.367812Z","iopub.status.idle":"2025-01-03T06:01:35.373321Z","shell.execute_reply.started":"2025-01-03T06:01:35.367788Z","shell.execute_reply":"2025-01-03T06:01:35.372436Z"}},"outputs":[],"execution_count":14},{"cell_type":"code","source":"import logging\nlogging.disable(logging.WARNING)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-02T16:10:53.265568Z","iopub.execute_input":"2025-01-02T16:10:53.265754Z","iopub.status.idle":"2025-01-02T16:10:53.280853Z","shell.execute_reply.started":"2025-01-02T16:10:53.265729Z","shell.execute_reply":"2025-01-02T16:10:53.280085Z"}},"outputs":[],"execution_count":16},{"cell_type":"markdown","source":"# Chunking for re-ranking","metadata":{}},{"cell_type":"code","source":"from nltk.tokenize import sent_tokenize","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-02T16:10:53.281610Z","iopub.execute_input":"2025-01-02T16:10:53.281750Z","iopub.status.idle":"2025-01-02T16:10:53.754526Z","shell.execute_reply.started":"2025-01-02T16:10:53.281733Z","shell.execute_reply":"2025-01-02T16:10:53.753661Z"}},"outputs":[],"execution_count":17},{"cell_type":"code","source":"from transformers import AutoTokenizer\n\n# Load the tokenizer\ntokenizer = AutoTokenizer.from_pretrained(\"vinai/phobert-base\")\n\n# Define the input text\ntext = \"\"\"BỘT CÁNH - CẲNG - BÀN TAY\\n...\\n2. Phương tiện\\n- 01 bàn nắn thông thường, tốt nhất như kiểu bàn mổ (chắc, nặng, để khi kéo nắn bàn không bị chạy). Ở nơi không có điều kiện, có thể dùng bàn sắt, bàn gỗ, nhưng chân bàn phải được cố định chắc xuống sàn nhà. Bàn kéo nắn cần có các mấu ngang để mắc các đai đối lực khi kéo nắn.\\n- Đai đối lực: bằng vải mềm, dai, to bản (như kiểu quai ba lô) để tránh gây tổn thương cho da khi kéo nắn.\\n- Thuốc tê hoặc thuốc mê: số lượng tùy thuộc người bệnh là trẻ em hay người lớn, trọng lượng người bệnh. Kèm theo là dụng cụ gây tê, gây mê, hồi sức (bơm tiêm, cồn 70o, thuốc chống sốc, mặt nạ bóp bóng, đèn nội khí quản...).\\n- Bột thạch cao chuyên dụng: với người lớn cần 2-3 cuộn cỡ 15 cm và 3-4 cuộn cỡ 10 cm là đủ (kể cả 1 phần trong đó dùng để rải thành nẹp bột, trẻ em dùng ít hơn tùy theo tuổi). Ở những tuyến có điều kiện, tốt nhất là dùng loại bột đóng gói sẵn. Ở những nơi không có điều kiện, sử dụng bột tự sản xuất cũng tốt, với điều kiện bột sản xuất ra không để quá lâu, bột sản xuất quá lâu sẽ bị bão hòa hơi nước, không đảm bảo độ vững chắc nữa, bột bó xong sẽ nhanh rã, nhanh hỏng.\\n- Giấy vệ sinh, bông cuộn hoặc bít tất vải xốp mềm để lót (jersey). Lưu ý, nếu dùng giấy vệ sinh, có thể gây dị ứng da người bệnh. Ở những nước phát triển, người ta thường dùng jersey, rất tiện lợi, vì nó có độ co giãn rất tốt, không gây chèn ép và có thể dùng cho nhiều kích cỡ chân tay to nhỏ khác nhau.\\n- Dây rạch dọc (dùng cho bột cấp cứu, khi tổn thương 7 ngày trở lại): thường dùng một đoạn băng vải có độ dài vừa phải, vê săn lại để đảm bảo độ chắc là đủ, không cần dây chuyên dụng.\\n- Dao hoặc cưa rung để rạch dọc bột trong trường hợp bó bột cấp cứu (tổn thương trong 7 ngày đầu). Nếu dùng dao rạch bột, dao cần sắc, nhưng không nên dùng dao mũi nhọn, đề phòng lỡ tay gây vết thương cho người bệnh (mặc dù tai biến này rất hiếm gặp). Nếu dùng cưa rung để rạch bột, cần lưu ý phải chờ cho bột khô hẳn mới làm, vì cưa rung chỉ cắt đứt các vật khô cứng, cũng chính vì thế chúng ta không lo ngại cưa rung làm rách da người bệnh, có chăng, nên cẩn thận khi cưa bột mà ngay ở dưới lưỡi cưa là các mấu xương (ví dụ các mắt cá, xương bánh chè...).\\n- Nước để ngâm bột: đủ về số lượng để ngâm chìm hẳn các cuộn bột. Lưu ý, mùa lạnh phải dùng nước ấm, vì trong quá trình bột khô cứng sẽ tiêu hao một nhiệt lượng đáng kể làm nóng bột, có thể làm hạ thân nhiệt người bệnh, gây cảm lạnh. Nước sử dụng ngâm bột phải được thay thường xuyên để đảm bảo vệ sinh và tránh hiện tượng nước có quá nhiều cặn bột ảnh hưởng đến chất lượng bột.\\n- 1 cuộn băng vải hoặc băng thun, để băng giữ ngoài bột, khi việc bó bột và rạch dọc bột đã hoàn thành.\\n... BỘT CÁNH - CẲNG - BÀN TAY\\n...\\n2. Phương tiện\\n- 01 bàn nắn thông thường, tốt nhất như kiểu bàn mổ (chắc, nặng, để khi kéo nắn bàn không bị chạy). Ở nơi không có điều kiện, có thể dùng bàn sắt, bàn gỗ, nhưng chân bàn phải được cố định chắc xuống sàn nhà. Bàn kéo nắn cần có các mấu ngang để mắc các đai đối lực khi kéo nắn.\\n- Đai đối lực: bằng vải mềm, dai, to bản (như kiểu quai ba lô) để tránh gây tổn thương cho da khi kéo nắn.\\n- Thuốc tê hoặc thuốc mê: số lượng tùy thuộc người bệnh là trẻ em hay người lớn, trọng lượng người bệnh. Kèm theo là dụng cụ gây tê, gây mê, hồi sức (bơm tiêm, cồn 70o, thuốc chống sốc, mặt nạ bóp bóng, đèn nội khí quản...).\\n- Bột thạch cao chuyên dụng: với người lớn cần 2-3 cuộn cỡ 15 cm và 3-4 cuộn cỡ 10 cm là đủ (kể cả 1 phần trong đó dùng để rải thành nẹp bột, trẻ em dùng ít hơn tùy theo tuổi). Ở những tuyến có điều kiện, tốt nhất là dùng loại bột đóng gói sẵn. Ở những nơi không có điều kiện, sử dụng bột tự sản xuất cũng tốt, với điều kiện bột sản xuất ra không để quá lâu, bột sản xuất quá lâu sẽ bị bão hòa hơi nước, không đảm bảo độ vững chắc nữa, bột bó xong sẽ nhanh rã, nhanh hỏng.\\n- Giấy vệ sinh, bông cuộn hoặc bít tất vải xốp mềm để lót (jersey). Lưu ý, nếu dùng giấy vệ sinh, có thể gây dị ứng da người bệnh. Ở những nước phát triển, người ta thường dùng jersey, rất tiện lợi, vì nó có độ co giãn rất tốt, không gây chèn ép và có thể dùng cho nhiều kích cỡ chân tay to nhỏ khác nhau.\\n- Dây rạch dọc (dùng cho bột cấp cứu, khi tổn thương 7 ngày trở lại): thường dùng một đoạn băng vải có độ dài vừa phải, vê săn lại để đảm bảo độ chắc là đủ, không cần dây chuyên dụng.\\n- Dao hoặc cưa rung để rạch dọc bột trong trường hợp bó bột cấp cứu (tổn thương trong 7 ngày đầu). Nếu dùng dao rạch bột, dao cần sắc, nhưng không nên dùng dao mũi nhọn, đề phòng lỡ tay gây vết thương cho người bệnh (mặc dù tai biến này rất hiếm gặp). Nếu dùng cưa rung để rạch bột, cần lưu ý phải chờ cho bột khô hẳn mới làm, vì cưa rung chỉ cắt đứt các vật khô cứng, cũng chính vì thế chúng ta không lo ngại cưa rung làm rách da người bệnh, có chăng, nên cẩn thận khi cưa bột mà ngay ở dưới lưỡi cưa là các mấu xương (ví dụ các mắt cá, xương bánh chè...).\\n- Nước để ngâm bột: đủ về số lượng để ngâm chìm hẳn các cuộn bột. Lưu ý, mùa lạnh phải dùng nước ấm, vì trong quá trình bột khô cứng sẽ tiêu hao một nhiệt lượng đáng kể làm nóng bột, có thể làm hạ thân nhiệt người bệnh, gây cảm lạnh. Nước sử dụng ngâm bột phải được thay thường xuyên để đảm bảo vệ sinh và tránh hiện tượng nước có quá nhiều cặn bột ảnh hưởng đến chất lượng bột.\\n- 1 cuộn băng vải hoặc băng thun, để băng giữ ngoài bột, khi việc bó bột và rạch dọc bột đã hoàn thành.\\n...\"\"\"\n\n# Parameters\nmax_seq_length = 1000\noverlap = 128\n\n# Tokenize the text\ntokens = tokenizer.tokenize(text)\n\n# Chunking logic\nchunks = []\nstart = 0\n\nwhile start < len(tokens):\n    end = min(start + max_seq_length, len(tokens))\n    chunks.append(tokens[start:end])\n    start += max_seq_length - overlap  # Move to the next chunk with overlap\n\n# Optional: Convert chunks back to text\nchunk_texts = [tokenizer.convert_tokens_to_string(chunk) for chunk in chunks]\n\n# Output the chunks\nfor i, chunk in enumerate(chunk_texts):\n    print(f\"Chunk {i+1}:\\n{preprocess_text(chunk)}\\n{'-'*50}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-02T16:10:53.755564Z","iopub.execute_input":"2025-01-02T16:10:53.755771Z","iopub.status.idle":"2025-01-02T16:10:58.670712Z","shell.execute_reply.started":"2025-01-02T16:10:53.755740Z","shell.execute_reply":"2025-01-02T16:10:58.669847Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/557 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7448e68a2cdc4b5b99f904377a71ee5b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.txt:   0%|          | 0.00/895k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ab74a1bdf7f640dd800463a538e690c8"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"bpe.codes:   0%|          | 0.00/1.14M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2db74d3537bd4abba92a3474585bc461"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/3.13M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5cf3ec4efa3d45edada7286d742bf456"}},"metadata":{}},{"name":"stdout","text":"Chunk 1:\nBỘT CÁNH CẲNG BÀN TAY ... Phương tiện 01 bàn nắn thông thường, tốt nhất như kiểu bàn mổ (chắc, nặng, để khi kéo nắn bàn không bị chạy). Ở nơi không có điều kiện, có thể dùng bàn sắt, bàn gỗ, nhưng chân bàn phải được cố định chắc xuống sàn nhà. Bàn kéo nắn cần có các mấu ngang để mắc các đai đối lực khi kéo nắn. Đai đối lực. bằng vải mềm, dai, to bản (như kiểu quai ba lô) để tránh gây tổn thương cho da khi kéo nắn. Thuốc tê hoặc thuốc mê. số lượng tùy thuộc người bệnh là trẻ em hay người lớn, trọng lượng người bệnh. Kèm theo là dụng cụ gây tê, gây mê, hồi sức (bơm tiêm, cồn 70o, thuốc chống sốc, mặt nạ bóp bóng, đèn nội khí quản...). Bột thạch cao chuyên dụng. với người lớn cần 23 cuộn cỡ 15 cm và 34 cuộn cỡ 10 cm là đủ (kể cả 1 phần trong đó dùng để rải thành nẹp bột, trẻ em dùng ít hơn tùy theo tuổi). Ở những tuyến có điều kiện, tốt nhất là dùng loại bột đóng gói sẵn. Ở những nơi không có điều kiện, sử dụng bột tự sản xuất cũng tốt, với điều kiện bột sản xuất ra không để quá lâu, bột sản xuất quá lâu sẽ bị bão hòa hơi nước, không đảm bảo độ vững chắc nữa, bột bó xong sẽ nhanh rã, nhanh hỏng. Giấy vệ sinh, bông cuộn hoặc bít tất vải xốp mềm để lót (jersey). Lưu ý, nếu dùng giấy vệ sinh, có thể gây dị ứng da người bệnh. Ở những nước phát triển, người ta thường dùng jersey, rất tiện lợi, vì nó có độ co giãn rất tốt, không gây chèn ép và có thể dùng cho nhiều kích cỡ chân tay to nhỏ khác nhau. Dây rạch dọc (dùng cho bột cấp cứu, khi tổn thương 7 ngày trở lại). thường dùng một đoạn băng vải có độ dài vừa phải, vê săn lại để đảm bảo độ chắc là đủ, không cần dây chuyên dụng. Dao hoặc cưa rung để rạch dọc bột trong trường hợp bó bột cấp cứu (tổn thương trong 7 ngày đầu). Nếu dùng dao rạch bột, dao cần sắc, nhưng không nên dùng dao mũi nhọn, đề phòng lỡ tay gây vết thương cho người bệnh (mặc dù tai biến này rất hiếm gặp). Nếu dùng cưa rung để rạch bột, cần lưu ý phải chờ cho bột khô hẳn mới làm, vì cưa rung chỉ cắt đứt các vật khô cứng, cũng chính vì thế chúng ta không lo ngại cưa rung làm rách da người bệnh, có chăng, nên cẩn thận khi cưa bột mà ngay ở dưới lưỡi cưa là các mấu xương (ví dụ các mắt cá, xương bánh chè...). Nước để ngâm bột. đủ về số lượng để ngâm chìm hẳn các cuộn bột. Lưu ý, mùa lạnh phải dùng nước ấm, vì trong quá trình bột khô cứng sẽ tiêu hao một nhiệt lượng đáng kể làm nóng bột, có thể làm hạ thân nhiệt người bệnh, gây cảm lạnh. Nước sử dụng ngâm bột phải được thay thường xuyên để đảm bảo vệ sinh và tránh hiện tượng nước có quá nhiều cặn bột ảnh hưởng đến chất lượng bột. 1 cuộn băng vải hoặc băng thun, để băng giữ ngoài bột, khi việc bó bột và rạch dọc bột đã hoàn thành. ... BỘT CÁNH CẲNG BÀN TAY ... Phương tiện 01 bàn nắn thông thường, tốt nhất như kiểu bàn mổ (chắc, nặng, để khi kéo nắn bàn không bị chạy). Ở nơi không có điều kiện, có thể dùng bàn sắt, bàn gỗ, nhưng chân bàn phải được cố định chắc xuống sàn nhà. Bàn kéo nắn cần có các mấu ngang để mắc các đai đối lực khi kéo nắn. Đai đối lực. bằng vải mềm, dai, to bản (như kiểu quai ba lô) để tránh gây tổn thương cho da khi kéo nắn. Thuốc tê hoặc thuốc mê. số lượng tùy thuộc người bệnh là trẻ em hay người lớn, trọng lượng người bệnh. Kèm theo là dụng cụ gây tê, gây mê, hồi sức (bơm tiêm, cồn 70o, thuốc chống sốc, mặt nạ bóp bóng, đèn nội khí quản...). Bột thạch cao chuyên dụng. với người lớn cần 23 cuộn cỡ 15 cm và 34 cuộn\n--------------------------------------------------\nChunk 2:\nắn. Đai đối lực. bằng vải mềm, dai, to bản (như kiểu quai ba lô) để tránh gây tổn thương cho da khi kéo nắn. Thuốc tê hoặc thuốc mê. số lượng tùy thuộc người bệnh là trẻ em hay người lớn, trọng lượng người bệnh. Kèm theo là dụng cụ gây tê, gây mê, hồi sức (bơm tiêm, cồn 70o, thuốc chống sốc, mặt nạ bóp bóng, đèn nội khí quản...). Bột thạch cao chuyên dụng. với người lớn cần 23 cuộn cỡ 15 cm và 34 cuộn cỡ 10 cm là đủ (kể cả 1 phần trong đó dùng để rải thành nẹp bột, trẻ em dùng ít hơn tùy theo tuổi). Ở những tuyến có điều kiện, tốt nhất là dùng loại bột đóng gói sẵn. Ở những nơi không có điều kiện, sử dụng bột tự sản xuất cũng tốt, với điều kiện bột sản xuất ra không để quá lâu, bột sản xuất quá lâu sẽ bị bão hòa hơi nước, không đảm bảo độ vững chắc nữa, bột bó xong sẽ nhanh rã, nhanh hỏng. Giấy vệ sinh, bông cuộn hoặc bít tất vải xốp mềm để lót (jersey). Lưu ý, nếu dùng giấy vệ sinh, có thể gây dị ứng da người bệnh. Ở những nước phát triển, người ta thường dùng jersey, rất tiện lợi, vì nó có độ co giãn rất tốt, không gây chèn ép và có thể dùng cho nhiều kích cỡ chân tay to nhỏ khác nhau. Dây rạch dọc (dùng cho bột cấp cứu, khi tổn thương 7 ngày trở lại). thường dùng một đoạn băng vải có độ dài vừa phải, vê săn lại để đảm bảo độ chắc là đủ, không cần dây chuyên dụng. Dao hoặc cưa rung để rạch dọc bột trong trường hợp bó bột cấp cứu (tổn thương trong 7 ngày đầu). Nếu dùng dao rạch bột, dao cần sắc, nhưng không nên dùng dao mũi nhọn, đề phòng lỡ tay gây vết thương cho người bệnh (mặc dù tai biến này rất hiếm gặp). Nếu dùng cưa rung để rạch bột, cần lưu ý phải chờ cho bột khô hẳn mới làm, vì cưa rung chỉ cắt đứt các vật khô cứng, cũng chính vì thế chúng ta không lo ngại cưa rung làm rách da người bệnh, có chăng, nên cẩn thận khi cưa bột mà ngay ở dưới lưỡi cưa là các mấu xương (ví dụ các mắt cá, xương bánh chè...). Nước để ngâm bột. đủ về số lượng để ngâm chìm hẳn các cuộn bột. Lưu ý, mùa lạnh phải dùng nước ấm, vì trong quá trình bột khô cứng sẽ tiêu hao một nhiệt lượng đáng kể làm nóng bột, có thể làm hạ thân nhiệt người bệnh, gây cảm lạnh. Nước sử dụng ngâm bột phải được thay thường xuyên để đảm bảo vệ sinh và tránh hiện tượng nước có quá nhiều cặn bột ảnh hưởng đến chất lượng bột. 1 cuộn băng vải hoặc băng thun, để băng giữ ngoài bột, khi việc bó bột và rạch dọc bột đã hoàn thành. ...\n--------------------------------------------------\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n  warnings.warn(\n","output_type":"stream"}],"execution_count":18},{"cell_type":"code","source":"import torch","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-02T16:10:58.671667Z","iopub.execute_input":"2025-01-02T16:10:58.671890Z","iopub.status.idle":"2025-01-02T16:10:58.675192Z","shell.execute_reply.started":"2025-01-02T16:10:58.671862Z","shell.execute_reply":"2025-01-02T16:10:58.674629Z"}},"outputs":[],"execution_count":19},{"cell_type":"code","source":"torch.cuda.empty_cache()\nquery = \"Thực phẩm ăn nhẹ phù hợp trước khi chạy bộ\"\nquery_embedding = model.encode(query)\nhits = util.semantic_search(query_embedding, embeddings, top_k=50)[0]\nprint(query)\nprint(\"\")\nprint(\"context: \")\nprint(\"\")\nfor hit in hits[:10]:\n    score = hit['score']\n    corpus_id = hit['corpus_id']\n    print(\"bi_score: \", score)\n\n    # print(str(cids[corpus_id].decode()))\n    # print(corpus.loc[(corpus['cid']) == (str(cids[corpus_id].decode())), 'text'].iloc[0])\n    print(corpus.iloc[corpus_id]['text'])\n    print(\" \")\n\ncross_inp = [[preprocess_text(query), preprocess_text(corpus.iloc[hit['corpus_id']]['text'])] for hit in hits]\n\ncross_scores = cross_encoder.predict(cross_inp)\nfor idx in range(len(hits)):\n    hits[idx]['cross-score'] = cross_scores[idx]\nhits = sorted(hits, key=lambda x: x['cross-score'], reverse=True)\nfor hit in hits[:10]:\n    score = hit['cross-score']\n    corpus_id = hit['corpus_id']\n    print(\"cross_score: \", score)\n\n    # print(str(cids[corpus_id].decode()))\n    # print(corpus.loc[(corpus['cid']) == (str(cids[corpus_id].decode())), 'text'].iloc[0])\n    print(corpus.iloc[corpus_id]['text'])\n    print(\" \")\n\ntorch.cuda.empty_cache()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!pip install pyngrok\n!ngrok config add-authtoken 2qORvsrf7DPupP5SxpxroXZcHrt_5wvaAdPmdAyqb7JHYYN21\n!pip install flask-cors","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-02T16:11:04.477131Z","iopub.execute_input":"2025-01-02T16:11:04.477364Z","iopub.status.idle":"2025-01-02T16:11:15.417889Z","shell.execute_reply.started":"2025-01-02T16:11:04.477330Z","shell.execute_reply":"2025-01-02T16:11:15.417170Z"}},"outputs":[{"name":"stderr","text":"/usr/lib/python3.10/pty.py:89: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  pid, fd = os.forkpty()\n","output_type":"stream"},{"name":"stdout","text":"Collecting pyngrok\n  Downloading pyngrok-7.2.2-py3-none-any.whl.metadata (8.4 kB)\nRequirement already satisfied: PyYAML>=5.1 in /usr/local/lib/python3.10/dist-packages (from pyngrok) (6.0.2)\nDownloading pyngrok-7.2.2-py3-none-any.whl (22 kB)\nInstalling collected packages: pyngrok\nSuccessfully installed pyngrok-7.2.2\nAuthtoken saved to configuration file: /root/.config/ngrok/ngrok.yml                                \nCollecting flask-cors\n  Downloading Flask_Cors-5.0.0-py2.py3-none-any.whl.metadata (5.5 kB)\nRequirement already satisfied: Flask>=0.9 in /usr/local/lib/python3.10/dist-packages (from flask-cors) (2.2.5)\nRequirement already satisfied: Werkzeug>=2.2.2 in /usr/local/lib/python3.10/dist-packages (from Flask>=0.9->flask-cors) (3.0.4)\nRequirement already satisfied: Jinja2>=3.0 in /usr/local/lib/python3.10/dist-packages (from Flask>=0.9->flask-cors) (3.1.4)\nRequirement already satisfied: itsdangerous>=2.0 in /usr/local/lib/python3.10/dist-packages (from Flask>=0.9->flask-cors) (2.2.0)\nRequirement already satisfied: click>=8.0 in /usr/local/lib/python3.10/dist-packages (from Flask>=0.9->flask-cors) (8.1.7)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from Jinja2>=3.0->Flask>=0.9->flask-cors) (2.1.5)\nDownloading Flask_Cors-5.0.0-py2.py3-none-any.whl (14 kB)\nInstalling collected packages: flask-cors\nSuccessfully installed flask-cors-5.0.0\n","output_type":"stream"}],"execution_count":21},{"cell_type":"code","source":"def cross_encoder_retrieval(cross_inp, hits):\n    cross_scores = cross_encoder.predict(cross_inp)\n    for idx in range(len(cross_scores)):\n        hits[idx]['cross-score'] = cross_scores[idx]\n    hits = sorted(hits, key=lambda x: x['cross-score'], reverse=True)\n    for hit in hits[:10]:\n        score = hit['cross-score']\n        corpus_id = hit['corpus_id']\n        print(\"cross_score: \", score)\n\n        print(corpus.iloc[corpus_id]['cid'])\n        print(corpus.iloc[corpus_id]['text'])\n        print(\" \")\n    best_hit = hits[0]\n    return corpus.iloc[best_hit['corpus_id']]['text']","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-02T16:11:15.419011Z","iopub.execute_input":"2025-01-02T16:11:15.419231Z","iopub.status.idle":"2025-01-02T16:11:15.424838Z","shell.execute_reply.started":"2025-01-02T16:11:15.419201Z","shell.execute_reply":"2025-01-02T16:11:15.424194Z"}},"outputs":[],"execution_count":22},{"cell_type":"code","source":"from flask import Flask, request, jsonify\nfrom pyngrok import ngrok\nfrom flask_cors import CORS\nimport requests  # Thư viện để gọi API HTTP\n\napp = Flask(__name__)\nCORS(app)\n\n# URL server ngrok thay vì sử dụng hàm `phogpt`\nNGROK_SERVER_URL = \"https://8250-115-78-13-71.ngrok-free.app\"  # Thay bằng URL ngrok của bạn\n\n@app.route('/predict', methods=['POST'])\ndef predict():\n    try:\n        # Lấy dữ liệu từ request.form\n        question = request.form.get('message')  # 'message' khớp với key trong JavaScript\n        if not question:\n            return jsonify({\"error\": \"Missing question\"}), 400\n\n        query_embedding = model.encode(question)\n        hits = util.semantic_search(query_embedding, embeddings, top_k=50)[0]\n        cross_inp = [[preprocess_text(question), preprocess_text(corpus.iloc[hit['corpus_id']]['text'])] for hit in hits]\n        cross_scores = cross_encoder.predict(cross_inp)\n        for idx in range(len(hits)):\n            hits[idx]['cross-score'] = cross_scores[idx]\n        hits = sorted(hits, key=lambda x: x['cross-score'], reverse=True)\n        best_hit = hits[0]\n        top_k_cross = corpus.iloc[best_hit['corpus_id']]['text']\n        print('cross-encoder')\n        \n        # Cross-encoder refinement\n        print(top_k_cross)\n        # Gọi tới server ngrok thay vì dùng phogpt\n        try:\n            response = requests.post(\n                f\"{NGROK_SERVER_URL}/ask\",  # Đường dẫn tới endpoint xử lý trên server ngrok\n                json={\n                    \"question\": question,\n                    \"context\": top_k_cross  # Gửi thông tin cần thiết\n                }\n            )\n            response_data = response.json()\n\n            # Kiểm tra phản hồi từ server\n            if response.status_code != 200 or \"answer\" not in response_data:\n                return jsonify({\"answer\": \"Error processing the request to the external server.\"})\n            \n            answer = response_data[\"answer\"]\n        except Exception as e:\n            print(f\"Error calling external server: {str(e)}\")\n            return jsonify({\"answer\": \"Error contacting external server.\"})\n\n        if not answer:\n            return jsonify({\"answer\": \"Sorry, I couldn't generate an appropriate response.\"})\n        return jsonify({\"answer\": answer})\n\n    except Exception as e:\n        # Log lỗi và trả về phản hồi JSON lỗi\n        print(f\"Error: {str(e)}\")\n        return jsonify({\"error\": f\"An error occurred: {str(e)}\"}), 500\n\n# Start the Flask server\nif __name__ == '__main__':\n    public_url = ngrok.connect(5000)  # Kết nối Flask server với ngrok\n    print(f\"Public URL: {public_url}\")\n    app.run(host='0.0.0.0', port=5000)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-02T16:11:15.425678Z","iopub.execute_input":"2025-01-02T16:11:15.425823Z","execution_failed":"2025-01-02T16:50:35.225Z"}},"outputs":[{"name":"stdout","text":"Public URL: NgrokTunnel: \"https://6b0c-35-189-4-26.ngrok-free.app\" -> \"http://localhost:5000\"\n * Serving Flask app '__main__'\n * Debug mode: off\n","output_type":"stream"}],"execution_count":null},{"cell_type":"code","source":"# torch.cuda.empty_cache()\n# query = \"trốn nghĩa vụ quân sự có đi tù không ?\"\n# query_embedding = model.encode(query)\n# hits = util.semantic_search(query_embedding, embeddings, top_k=100)[0]\n# for hit in hits[:10]:\n#     score = hit['score']\n#     corpus_id = hit['corpus_id']\n#     print(\"bi_score: \", score)\n\n#     print(str(cids[corpus_id].decode()))\n#     # print(corpus.loc[(corpus['cid']) == (str(cids[corpus_id].decode())), 'text'].iloc[0])\n#     print(corpus.iloc[corpus_id]['text'])\n#     print(\" \")\n\n# cross_inp = [[preprocess_text(query), preprocess_text(corpus.iloc[hit['corpus_id']]['text'])] for hit in hits]\n\n# cross_scores = cross_encoder_og.predict(cross_inp)\n# for idx in range(len(hits)):\n#     hits[idx]['cross-score'] = cross_scores[idx]\n# hits = sorted(hits, key=lambda x: x['cross-score'], reverse=True)\n# for hit in hits[:10]:\n#     score = hit['cross-score']\n#     corpus_id = hit['corpus_id']\n#     print(\"cross_score: \", score)\n\n#     print(str(cids[corpus_id].decode()))\n#     # print(corpus.loc[(corpus['cid']) == (str(cids[corpus_id].decode())), 'text'].iloc[0])\n#     print(corpus.iloc[corpus_id]['text'])\n#     print(\" \")\n\n# torch.cuda.empty_cache()","metadata":{"trusted":true,"execution":{"execution_failed":"2025-01-02T16:50:35.226Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# import random\n# from sentence_transformers import SentenceTransformer\n# from sentence_transformers.evaluation import InformationRetrievalEvaluator\n# from datasets import load_dataset","metadata":{"trusted":true,"execution":{"execution_failed":"2025-01-02T16:50:35.226Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# corpus_eval = load_dataset(\"hiieu/legal_eval\", split=\"corpus\")\n# queries_eval = load_dataset(\"hiieu/legal_eval\", split=\"queries\")\n# relevant_docs_data = load_dataset(\"hiieu/legal_eval_label\", split=\"train\")\n# # Convert the datasets to dictionaries\n# corpus_eval = dict(zip(corpus_eval[\"id\"], corpus_eval[\"text\"]))  # Our corpus (cid => document)\n# queries_eval = dict(zip(queries_eval[\"id\"], queries_eval[\"text\"]))  # Our queries (qid => question)\n# relevant_docs = {}  # Query ID to relevant documents (qid => set([relevant_cids])\n# for qid, corpus_ids in zip(relevant_docs_data[\"question_id\"], relevant_docs_data[\"corpus_id\"]):\n#     qid = str(qid)\n#     corpus_ids = str(corpus_ids)\n#     if qid not in relevant_docs:\n#         relevant_docs[qid] = set()\n#     relevant_docs[qid].add(corpus_ids)","metadata":{"trusted":true,"execution":{"execution_failed":"2025-01-02T16:50:35.226Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# import json\n# import torch\n# from sentence_transformers import SentenceTransformer\n# from sentence_transformers.evaluation import (\n#     InformationRetrievalEvaluator,\n#     SequentialEvaluator,\n# )\n# from sentence_transformers.util import cos_sim\n# from datasets import load_dataset, concatenate_datasets\n\n# matryoshka_dimensions = [768] # Important: large to small\n# matryoshka_evaluators = []\n# # Iterate over the different dimensions\n# for dim in matryoshka_dimensions:\n#     ir_evaluator = InformationRetrievalEvaluator(\n#         queries=queries_eval,\n#         corpus=corpus_eval,\n#         relevant_docs=relevant_docs,\n#         name=f\"dim_{dim}\",\n#         truncate_dim=dim,  # Truncate the embeddings to a certain dimension\n#         score_functions={\"cosine\": cos_sim},\n#     )\n#     matryoshka_evaluators.append(ir_evaluator)\n\n# # Create a sequential evaluator\n# evaluator = SequentialEvaluator(matryoshka_evaluators)","metadata":{"trusted":true,"execution":{"execution_failed":"2025-01-02T16:50:35.226Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# # Evaluate the model\n# results = evaluator(model)\n# for k,v in results.items():\n#     print(k, v)","metadata":{"trusted":true,"execution":{"execution_failed":"2025-01-02T16:50:35.226Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# model_m3 = SentenceTransformer(\"/kaggle/input/bge_m3_finetuned/transformers/default/1/checkpoint-5000\")","metadata":{"trusted":true,"execution":{"execution_failed":"2025-01-02T16:50:35.226Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# results = evaluator(model)\n# for k,v in results.items():\n#     print(k, v)","metadata":{"trusted":true,"execution":{"execution_failed":"2025-01-02T16:50:35.226Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# import pandas as pd\n# import nltk\n# from nltk.tokenize import word_tokenize\n\n# # Download the punkt tokenizer if not already downloaded\n# nltk.download('punkt')\n\n# # Load the CSV file into a DataFrame\n# df = pd.read_csv('/kaggle/input/corpus-after-preprocessed-18de/corpus_after_preprocessed_18Dec.csv')  # Replace 'your_file.csv' with your actual file path\n\n# # Specify the column name containing the text data\n# text_column = 'text'  # Replace 'your_column_name' with the actual column name\n\n# # Ensure the specified column exists in the DataFrame\n# if text_column not in df.columns:\n#     raise ValueError(f\"Column '{text_column}' not found in the CSV file.\")\n\n# # Tokenize the text in each row of the specified column and count the tokens\n# token_counts = df[text_column].dropna().apply(lambda text: len(word_tokenize(text)))\n\n# # Calculate the maximum and mean token counts\n# max_tokens = token_counts.max()\n# mean_tokens = token_counts.mean()\n\n# # Print the results\n# print(f\"Maximum number of tokens in a single entry: {max_tokens}\")\n# print(f\"Mean number of tokens across all entries: {mean_tokens:.2f}\")\n","metadata":{"trusted":true,"execution":{"execution_failed":"2025-01-02T16:50:35.227Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# evaluate","metadata":{}},{"cell_type":"code","source":"# import pandas as pd\n# import numpy as np\n\n# # Đọc file predict.zip\n\n# def read_predict_file(file_path):\n#     predictions = {}\n#     with open(file_path, 'r') as f:\n#         for line in f:\n#             parts = line.strip().split()\n#             qid = parts[0]\n#             cids = [cid.split('_')[0] for cid in parts[1:]]\n#             predictions[qid] = cids\n#     return predictions\n\n# def parse_context(text):\n#     result = []\n#     parts = text.strip(\"[]\").split(\"\\n\")\n#     for part in parts:\n#         part = part.strip()\n#         if \"'\" in part:\n#             result.append(part.strip(\"'\"))\n#         else:\n#             result.extend(part.split())\n#     return result\n\n# # Đọc file CSV gốc (9456 sample cuối)\n# def read_ground_truth(train):\n#     # Lấy 9456 sample cuối\n#     df = train.head(9456)\n    \n#     # Tạo từ điển để tra cứu nhanh\n#     ground_truth = {}\n#     for _, row in df.iterrows():\n#         ground_truth[str(row['qid'])] = row['cid']\n    \n#     return ground_truth\n\n# # Tính MRR@10\n# def calculate_mrr(predictions, ground_truth):\n#     mrr_scores = []\n#     for qid, label_list in ground_truth.items():\n#         if qid in predictions:\n#             predicted_cids = [str(cid) for cid in predictions[qid]]\n#             rank = None\n#             for i, cid in enumerate(predicted_cids):\n#                 if cid in [str(label) for label in label_list]:\n#                     rank = i + 1\n#                     break\n#             if rank is not None:\n#                 mrr_scores.append(1 / rank)\n#             else:\n#                 mrr_scores.append(0)\n#         else:\n#             mrr_scores.append(0)\n    \n#     return np.mean(mrr_scores)\n\n# # Đường dẫn file\n# predict_file = '/kaggle/input/query-ela-jin/predict.zip'  # Hoặc 'predict.txt' tùy cách bạn lưu\n# csv_file = '/kaggle/input/query-ela-jin/train_csv.csv'","metadata":{"trusted":true,"execution":{"execution_failed":"2025-01-02T16:50:35.227Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# import py_vncorenlp\n# rdrsegmenter = py_vncorenlp.VnCoreNLP(annotators=[\"wseg\"], save_dir='/kaggle/input/vn-core-nlp/VnCoreNLP-master')","metadata":{"trusted":true,"execution":{"execution_failed":"2025-01-02T16:50:35.227Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# train = pd.read_csv(\"/kaggle/input/train-after-preprocessed-18dec/train_after_preprocessed_18Dec.csv\")\n# train['cid'] = train['cid'].apply(lambda x: x.strip(\"[]\"))\n# train['cid'] = train['cid'].astype(str)\n# train_subset = train.iloc[90000:95000]","metadata":{"trusted":true,"execution":{"execution_failed":"2025-01-02T16:50:35.227Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# train_subset.info()","metadata":{"trusted":true,"execution":{"execution_failed":"2025-01-02T16:50:35.227Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# from tqdm import tqdm","metadata":{"trusted":true,"execution":{"execution_failed":"2025-01-02T16:50:35.227Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# count = 0\n# total_queries = len(train_subset)\n# for test_case in tqdm(range(total_queries)):\n#     query = train_subset.iloc[test_case]['question']\n#     print(query)\n#     if count == 10:\n#         break\n#     count += 1","metadata":{"trusted":true,"execution":{"execution_failed":"2025-01-02T16:50:35.227Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# def calculate_mrr_at_k(test_data, k=50):\n#     \"\"\"\n#     Calculate MRR@K based on the re-ranked document IDs.\n\n#     Parameters:\n#         test_data (list): List of test cases in the format [{'query': str, 'cid': int, 'top_k_cross': list}, ...].\n#                          Each `top_k_cross` is a list of cids (document IDs) ranked by the cross-encoder.\n#         k (int): Number of top documents to consider for MRR@K.\n\n#     Returns:\n#         float: MRR@K score for the test dataset.\n#     \"\"\"\n#     mrr_score = 0.0\n#     total_queries = len(test_data)\n\n#     for test_case in tqdm(range(total_queries)):\n#         query = test_data.iloc[test_case]['question']\n#         top_k = 50\n#         query_embedding = model.encode(query)\n        \n#         hits = util.semantic_search(query_embedding, embeddings, top_k=top_k)[0]\n\n#         top_k_bi = []\n#         for hit in hits:\n#             top_k_bi.append(str(cids[hit['corpus_id']].decode()))\n        \n#         # cross_inp = [[query, preprocess_text(corpus.iloc[hit['corpus_id']]['context'])] for hit in hits]\n#         # cross_scores = cross_encoder.predict(cross_inp)\n#         # for idx in range(len(hits)):\n#         #     hits[idx]['cross-score'] = cross_scores[idx]\n#         # hits = sorted(hits, key=lambda x: x['cross-score'], reverse=True)\n#         # top_k_cross = cids[corpus_id].decode()\n\n#         if len(test_data.iloc[test_case]['cid']) > 1:\n#             true_cid = test_data.iloc[test_case]['cid']\n        \n#         # Calculate Reciprocal Rank (RR) for this query\n#         rr = 0.0\n#         for rank, cid in enumerate(top_k_bi, start=1):\n#             if cid == true_cid:\n#                 rr = 1 / rank\n#                 break\n\n#         mrr_score += rr\n\n#     # Compute the Mean Reciprocal Rank (MRR@K)\n#     mrr_score /= total_queries\n#     return mrr_score","metadata":{"trusted":true,"execution":{"execution_failed":"2025-01-02T16:50:35.227Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# bi_score = calculate_mrr_at_k(train_subset)","metadata":{"trusted":true,"execution":{"execution_failed":"2025-01-02T16:50:35.227Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# print(\"halong_embedding_finetuned\")\n# print(f\"MRR@10: {bi_score}\")","metadata":{"trusted":true,"execution":{"execution_failed":"2025-01-02T16:50:35.227Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# bi_score = calculate_mrr_at_k(train_subset)\n# print(\"halong_embedding_finetuned\")\n# print(f\"MRR@10: {bi_score}\")","metadata":{"trusted":true,"execution":{"execution_failed":"2025-01-02T16:50:35.227Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# print(\"halong_embedding_finetuned\")\n# print(f\"MRR@50: {bi_score}\")","metadata":{"trusted":true,"execution":{"execution_failed":"2025-01-02T16:50:35.227Z"}},"outputs":[],"execution_count":null}]}